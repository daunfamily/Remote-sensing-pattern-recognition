{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LE-NET5-tang",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPO1YZsY88Xwh9bWhtNpm7J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shz1236/Remote-sensing-pattern-recognition/blob/master/LE_NET5_tang.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCU72p5ib5BC",
        "colab_type": "code",
        "outputId": "3c5ac914-2440-4524-eb19-cbd9408599e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhfZ2BpZe5eC",
        "colab_type": "code",
        "outputId": "40562391-ee74-47f7-80f6-a7613e48b10f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pip install tensorflow-gpu"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/93/c7bca39b23aae45cd2e85ad3871c81eccc63b9c5276e926511e2e5b0879d/tensorflow_gpu-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 30kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.33.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.1.8)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.9.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.4.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/23/53ffe290341cd0855d595b0a2e7485932f473798af173bbe3a584b99bb06/tensorboard-2.1.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 41.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.1)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 59.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.17.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.11.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.10.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (42.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2.21.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.1.1)\n",
            "Collecting google-auth<2,>=1.6.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/6d/7aae38a9022f982cf8167775c7fc299f203417b698c27080ce09060bba07/google_auth-1.11.0-py2.py3-none-any.whl (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 12.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu) (2.8.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (4.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (4.0.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.2.7)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.4.8)\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorboard 2.1.0 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.11.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: google-auth, tensorboard, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: google-auth 1.4.2\n",
            "    Uninstalling google-auth-1.4.2:\n",
            "      Successfully uninstalled google-auth-1.4.2\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "Successfully installed google-auth-1.11.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FE8PYyH7bABm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a9d953b8-2b32-4027-faaf-68609e8a6cf7"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "small_batch= True\n",
        "\n",
        "num_classes=10\n",
        "\n",
        "learning_rate = 0.01\n",
        "training_steps = 1000\n",
        "batch_size=500\n",
        "\n",
        "conv1_filters = 6\n",
        "conv2_filters = 16\n",
        "conv3_filters = 120\n",
        "fc1_units = 84\n",
        "\n",
        "optimizer = tf.optimizers.SGD(learning_rate)\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/\")\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
        "x_train, x_test = x_train / 255., x_test / 255.\n",
        "\n",
        "\n",
        "\n",
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_data = train_data.repeat().batch(batch_size).prefetch(1)\n",
        "\n",
        "def conv2d(x, W, b, strides=1,padding='SAME'):\n",
        "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding=padding)\n",
        "    x = tf.nn.bias_add(x, b)\n",
        "    return x\n",
        "\n",
        "def maxpool2d(x, k=2,padding='SAME'):\n",
        "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
        "\n",
        "weights,biases=np.load('WB.npy',allow_pickle=True)\n",
        "\n",
        "# random_normal = tf.initializers.RandomNormal()\n",
        "# weights = {\n",
        "#     'wc1': tf.Variable(random_normal([5, 5, 1, conv1_filters])),\n",
        "#     'wc2': tf.Variable(random_normal([5, 5, conv1_filters, conv2_filters])),\n",
        "#     'wc3': tf.Variable(random_normal([5, 5, conv2_filters, conv3_filters])),\n",
        "\n",
        "#     'wd1': tf.Variable(random_normal([conv3_filters, fc1_units])),\n",
        "#     'out': tf.Variable(random_normal([fc1_units, num_classes]))\n",
        "# }\n",
        "\n",
        "# biases = {\n",
        "#     'bc1': tf.Variable(tf.zeros([conv1_filters])),\n",
        "#     'bc2': tf.Variable(tf.zeros([conv2_filters])),\n",
        "#     'bc3': tf.Variable(tf.zeros([conv3_filters])),\n",
        "\n",
        "#     'bd1': tf.Variable(tf.zeros([fc1_units])),\n",
        "#     'out': tf.Variable(tf.zeros([num_classes]))\n",
        "# }\n",
        "\n",
        "# np.save('WB.npy',(weights,biases),allow_pickle=True)\n",
        "\n",
        "    \n",
        "def bincode(input,output):\n",
        "    input=tf.where(input>0,1.0,0.0)\n",
        "    output_bincode=tf.where(output>0,1.0,0.0)\n",
        "    output_bincode=tf.expand_dims(output_bincode,3)\n",
        "    output_bincode=tf.tile(output_bincode, multiples=[1,1,1,input.shape[-1],1])\n",
        "    layer_bincode=np.array([[x,y] for x,y in zip(input,output_bincode)])\n",
        "    layer_bincode=np.apply_along_axis(lambda x : tf.nn.conv2d(np.expand_dims(x[0],0) ,x[1],strides=[1, 1, 1, 1], padding='VALID'),1,layer_bincode)\n",
        "    layer_bincode=tf.where(layer_bincode==0.0,5.0,layer_bincode)\n",
        "    layer_bincode=tf.tile(tf.transpose(layer_bincode,perm=[0,2,3,1,4]),multiples=[1,1,1,input.shape[-1],1])\n",
        "    return layer_bincode\n",
        "\n",
        "def cross_entropy(y_pred, y_true):\n",
        "    y_true = tf.one_hot(y_true, depth=num_classes)\n",
        "    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n",
        "    return y_true * tf.math.log(y_pred)\n",
        "\n",
        "    # return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred)))\n",
        "\n",
        "def accuracy(y_pred, y_true):\n",
        "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
        "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)\n",
        "\n",
        "def run_optimization(x, y):\n",
        "    trainable_variables = list(weights.values()) + list(biases.values())\n",
        "    with tf.GradientTape(persistent=True) as g:\n",
        "            x = tf.reshape(x, [-1, 28, 28, 1])\n",
        "            x = tf.pad(x,[[0,0],[2,2],[2,2],[0,0]])\n",
        "\n",
        "            conv1 = conv2d(x, weights['wc1'], biases['bc1'],padding='VALID')\n",
        "            act1 = tf.nn.relu(conv1)\n",
        "            pool1 = maxpool2d(act1, k=2)\n",
        "\n",
        "            conv2 = conv2d(pool1, weights['wc2'], biases['bc2'],padding='VALID')\n",
        "            act2 = tf.nn.relu(conv2) \n",
        "            pool2 = maxpool2d(act2, k=2)\n",
        "\n",
        "            conv3 = conv2d(pool2, weights['wc3'], biases['bc3'],padding='VALID')\n",
        "            act3 = tf.nn.relu(conv3)\n",
        "\n",
        "            fc1 = tf.reshape(act3, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
        "            fc1_bincode=tf.reduce_sum(tf.where(fc1>0 ,1.0,0.0),axis=0)\n",
        "\n",
        "            fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
        "            fc1 = tf.nn.relu(fc1)\n",
        "            fc1_bincode=tf.matmul(tf.expand_dims(fc1_bincode,axis=1), tf.expand_dims(tf.ones_like(biases['bd1']),axis=0))\n",
        "            fc1_bincode=tf.where(fc1_bincode==0,5,fc1_bincode)\n",
        "            out_bincode=tf.reduce_sum(tf.where(fc1>0 ,1.0,0.0),axis=0)\n",
        "\n",
        "            out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
        "            out = tf.nn.softmax(out)\n",
        "            out_bincode=tf.matmul(tf.expand_dims(out_bincode,axis=1), tf.expand_dims(tf.ones_like(biases['out']),axis=0))\n",
        "            out_bincode=tf.where(out_bincode==0,5,out_bincode)\n",
        "           \n",
        "            loss = -tf.reduce_sum(cross_entropy(out, y),axis=1)\n",
        "            gradients=list(map(lambda x:list(map(lambda x:x.numpy(),g.gradient(x, trainable_variables))),loss))\n",
        "    conv1_delta = g.gradient(loss, conv1)\n",
        "    conv2_delta = g.gradient(loss, conv2)\n",
        "    conv3_delta = g.gradient(loss, conv3)\n",
        "    wc1_bincode=bincode(x,conv1_delta)\n",
        "    wc2_bincode=bincode(pool1,conv2_delta)\n",
        "    wc3_bincode=bincode(pool2,conv3_delta)\n",
        "    layers_bincode=(wc1_bincode,wc2_bincode,wc3_bincode,fc1_bincode,out_bincode)\n",
        "    gradient = g.gradient(loss, trainable_variables)\n",
        "\n",
        "    if small_batch==True:\n",
        "        wc1_bincode=tf.reduce_sum(np.array([x[0] for x in gradients])/layers_bincode[0],0)\n",
        "        wc2_bincode=tf.reduce_sum(np.array([x[1] for x in gradients])/layers_bincode[1],0)\n",
        "        wc3_bincode=tf.reduce_sum(np.array([x[2] for x in gradients])/layers_bincode[2],0)\n",
        "        gradient[0:3]=[wc1_bincode,wc2_bincode,wc3_bincode]\n",
        "        gradient[3:5]=[tf.divide(x,y) for x,y in zip(gradient[3:5],layers_bincode[3:5])]\n",
        "    else:\n",
        "        gradient[0:5]=[tf.divide(x,batch_size) for x in gradient[0:5]]\n",
        "              \n",
        "    optimizer.apply_gradients(zip(gradient, trainable_variables))\n",
        "    return out"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FDucceZbyDZ",
        "colab_type": "code",
        "outputId": "f2cf59a9-8a79-49ea-dde4-84f0e948d635",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "loss_list=[]\n",
        "acc_list=[]\n",
        "for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n",
        "    pred=run_optimization(batch_x, batch_y)\n",
        "    loss = tf.reduce_mean(-tf.reduce_sum(cross_entropy(pred, batch_y)))\n",
        "    acc = accuracy(pred, batch_y)\n",
        "    print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))\n",
        "    loss_list.append(loss)\n",
        "    acc_list.append(acc)\n",
        "\n",
        "if  small_batch==True:\n",
        "\n",
        "    np.save('training_accuracy_small_batchsize_epoch=%d_batchsize=%d_learn_rate=%f.npy' % (\n",
        "                training_steps, batch_size, learning_rate),\n",
        "            acc_list, allow_pickle=True)\n",
        "    np.save('training_cost_small_batchsize_epoch=%d_batchsize=%d_learn_rate=%f.npy' % (\n",
        "                training_steps, batch_size, learning_rate),\n",
        "            loss_list, allow_pickle=True)\n",
        "else:\n",
        "    np.save('training_accuracy_normal_batchsize_epoch=%d_batchsize=%d_learn_rate=%f.npy' % (\n",
        "                training_steps, batch_size, learning_rate),\n",
        "            acc_list, allow_pickle=True)\n",
        "    np.save('training_cost_normal_batchsize_epoch=%d_batchsize=%d_learn_rate=%f.npy' % (\n",
        "                training_steps, batch_size, learning_rate),\n",
        "            loss_list, allow_pickle=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "step: 1, loss: 1152.089600, accuracy: 0.072000\n",
            "step: 2, loss: 1152.331055, accuracy: 0.100000\n",
            "step: 3, loss: 1148.860352, accuracy: 0.120000\n",
            "step: 4, loss: 1151.642090, accuracy: 0.094000\n",
            "step: 5, loss: 1150.067627, accuracy: 0.138000\n",
            "step: 6, loss: 1149.866821, accuracy: 0.090000\n",
            "step: 7, loss: 1151.921631, accuracy: 0.104000\n",
            "step: 8, loss: 1149.240479, accuracy: 0.088000\n",
            "step: 9, loss: 1148.140625, accuracy: 0.180000\n",
            "step: 10, loss: 1145.484619, accuracy: 0.112000\n",
            "step: 11, loss: 1147.256836, accuracy: 0.102000\n",
            "step: 12, loss: 1146.739258, accuracy: 0.102000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}